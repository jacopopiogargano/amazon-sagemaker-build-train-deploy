{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting, Exploring, Transforming data and features\n",
    "### [Amazon SageMaker Data Wranger](https://aws.amazon.com/sagemaker/data-wrangler/) & [Amazon SageMaker Feature Store](https://aws.amazon.com/sagemaker/feature-store/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the needed datasets, the next step in the Machine Learning workflow is to explore and preprocess data.\n",
    "\n",
    "In this notebook we will see how to run an Amazon SageMaker Data Wrangler job (implemented via Amazon SageMaker Processing) that will execute the transformations defined in the Data Wrangler flow and export the transformed data to Amazon SageMaker Feature Store.\n",
    "\n",
    "To do so we:\n",
    "1. create a feature group in Amazon Feature Store, to store the features describing the records;\n",
    "2. enable both offline and online feature store\n",
    "3. explore the data through Amazon SageMaker Data Wrangler\n",
    "4. define the data transformations and download the resulting `.flow` file\n",
    "5. run a Processing job to transform the data through the transformations defined in the `.flow` file\n",
    "6. output the transformed data to the feature group defined at 1.\n",
    "7. read the features from Amazon SageMaker Feature Store offline store through Amazon Athena\n",
    "8. output the features as CSV on S3 to be later used at training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check SageMaker Python SDK version\n",
    "import sagemaker\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "def versiontuple(v):\n",
    "    return tuple(map(int, (v.split(\".\"))))\n",
    "\n",
    "if versiontuple(sagemaker.__version__) < versiontuple('2.22.0'):\n",
    "    raise Exception(\"This notebook requires at least SageMaker Python SDK version 2.22.0. Please install it via pip.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'endtoendmlsm'\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started with preprocessing and feature engineering, we want to leverage on Amazon SageMaker Experiments to track the experimentations that we will be executing.\n",
    "We are going to create a new experiment and then a new trial, that represents a multi-step ML workflow (e.g. preprocessing stage1, preprocessing stage2, training stage, etc.). Each step of a trial maps to a trial component in SageMaker Experiments.\n",
    "\n",
    "We will use the Amazon SageMaker Experiments SDK to interact with the service from the notebooks. Additional info and documentation is available here: https://github.com/aws/sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are creating the experiment, or loading if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from smexperiments import experiment\n",
    "\n",
    "experiment_name = '{0}-{1}'.format(prefix, str(int(time.time())))\n",
    "current_experiment = experiment.Experiment.create(experiment_name=experiment_name,\n",
    "                                                  description='SageMaker workshop experiment')\n",
    "\n",
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our experiment, we can create a new trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = 'trial-{0}'.format(str(int(time.time())))\n",
    "current_trial = current_experiment.create_trial(trial_name=trial_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now own, we will use the experiment and the trial as configuration parameters for the preprocessing and training jobs, to make sure we track executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store experiment_name\n",
    "%store trial_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now copy to our bucket the dataset used for this use case. We will use the `windturbine_raw_data_header.csv` made available for this workshop in the `gianpo-public` public S3 bucket. In this Notebook, we will download from that bucket and upload to your bucket so that AWS services can access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "file_key = 'data/raw/windturbine_raw_data_header.csv'\n",
    "copy_source = {\n",
    "    'Bucket': 'gianpo-public',\n",
    "    'Key': 'endtoendml/{0}'.format(file_key)\n",
    "}\n",
    "\n",
    "s3.Bucket(bucket_name).copy(copy_source, '{0}/'.format(prefix) + file_key)\n",
    "\n",
    "print(f'Data saved to s3: {bucket_name}/{file_key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Feature Store\n",
    "\n",
    "First, let's create the target feature group in Amazon SageMaker Feature Store. A feature group is a logical grouping of features, defined in the feature store, to describe records. A feature group’s definition is composed of a list of feature definitions, a record identifier name, and configurations for its online and offline store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "feature_group_name = 'endtoendml-feature-group-{0}'.format(str(int(time.time())))\n",
    "print(feature_group_name)\n",
    "\n",
    "%store feature_group_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the schema for the feature group, by using an empty Pandas data frame. You can also infer it by reading some data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_columns = [\"breakdown\", \"wind_speed\", \"rpm_blade\", \"oil_temperature\", \"oil_level\", \"temperature\", \"humidity\", \n",
    "              \"vibrations_frequency\", \"pressure\", \"turbine_id_TID004\", \"turbine_id_TID001\", \"turbine_id_TID006\", \"turbine_id_TID008\", \n",
    "              \"turbine_id_TID002\", \"turbine_id_TID003\", \"turbine_id_TID005\", \"turbine_id_TID009\", \"turbine_id_TID010\", \"turbine_id_TID007\",\n",
    "              \"turbine_type_HAWT\",\"turbine_type_VAWT\", \"wind_direction_S\", \"wind_direction_N\", \"wind_direction_W\", \"wind_direction_SW\", \n",
    "              \"wind_direction_E\", \"wind_direction_SE\", \"wind_direction_NE\", \"wind_direction_NW\", \"record_id\", \"event_timestamp\"]\n",
    "df_schema = pd.DataFrame(columns=df_columns, dtype=float)\n",
    "# explicitly specifying data types for variables that are not floats\n",
    "df_schema = df_schema.astype({'wind_speed': 'long', 'rpm_blade': 'long', 'oil_level': 'long', 'temperature': 'long', 'humidity': 'long',\n",
    "                             'vibrations_frequency': 'long', 'pressure': 'long', 'record_id': 'long', 'event_timestamp': 'string'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the feature group specifying its name and the SageMaker session, and then we specify the schema for the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create the feature store; we will enable both online and offline store for this example.\n",
    "\n",
    "_Online store_: used for low latency real-time inference use cases (low millisecond latency reads and high throughput writes).\n",
    "\n",
    "_Offline store_: used for training and batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store import feature_group\n",
    "\n",
    "feature_group = feature_group.FeatureGroup(name=feature_group_name,\n",
    "                                           sagemaker_session = sagemaker_session)\n",
    "\n",
    "feature_group.load_feature_definitions(df_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we specify an s3 location for the offline feature store.\n",
    "offline_store_uri = 's3://{0}/{1}/feature_store'.format(bucket_name, prefix)\n",
    "\n",
    "feature_group.create(s3_uri=offline_store_uri,\n",
    "                     record_identifier_name='record_id',\n",
    "                     event_time_feature_name='event_timestamp',\n",
    "                     role_arn=role,\n",
    "                     enable_online_store=True,\n",
    "                     description='Wind turbine features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait a few seconds for the feature group to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    status = feature_group.describe()['FeatureGroupStatus']\n",
    "    print(status)\n",
    "    if status == 'Created':\n",
    "        break;\n",
    "    time.sleep(5)\n",
    "\n",
    "print(feature_group.athena_query())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Data Wrangler\n",
    "\n",
    "Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high quality models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, follow these steps:\n",
    "1. In the left menu, go to SageMaker resources (orange triangle shape)\n",
    "2. Select 'Data Wrangler'\n",
    "3. Create a New Flow and click on Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../images/wrangler_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Import the data after briefly inspecting it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../images/wrangler_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Click on the + symbol on the right and add an Analysis to explore the data through Data Wrangler's features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../images/wrangler_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. For instance, you may choose the Histogram visualization and plot the `wind_speed` distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../images/wrangler_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Go back to the data flow and add a Transform. There are many pre built transforms to choose from, plus you can bring your own transform or formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../images/wrangler_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. As an example, choose `Handle Missing` -> `Fill missing` -> `turbine_type` -> `HAWT`, and preview the transformation by clicking on __Preview__. The missing values in the column `turbine_type` were filled in with the string `HAWT`. If you are satisfied of the results, you can add the transform step to the transformation pipeline by clicking on __Add__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../images/wrangler_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. After you have added all the needed steps, you are all set. You may explore the `.flow` generated file in your local SageMaker repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../images/wrangler_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to use an Amazon SageMaker Data Wrangler job, implemented as a SageMaker Processing job, to interpret the data flow defined with Amazon SageMaker Data Wrangler and load the transformed data to the feature group previously created.\n",
    "\n",
    "First thing to do is uploading the data flow to Amazon S3, since it will be used as input to the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "flow_file_name = 'source_dir/data_exploration.flow'\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "    \n",
    "data_flow_uri = 's3://{0}/{1}/data_flow/{2}'.format(bucket_name, prefix, flow_file_name)\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket_name, '{0}/data_flow/{1}'.format(prefix, flow_file_name))\n",
    "\n",
    "print(data_flow_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker import image_uris\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/ecr-us-east-1.html#data-wrangler-us-east-1.title\n",
    "\n",
    "data_wrangler_image_uri = image_uris.retrieve(framework='data-wrangler',region=region, version='1.x')\n",
    "\n",
    "processor = Processor(image_uri=data_wrangler_image_uri,\n",
    "                      role=role,\n",
    "                      instance_count=1,\n",
    "                      instance_type='ml.m5.4xlarge',\n",
    "                      base_job_name='endtoendml-load-featurestore',\n",
    "                      sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the inputs for the Data Wrangler job. It expects the flow definition and all dataset definitions used to laod data in the flow. In this scenario, we only accessed a dataset from S3, so we are going to parse only S3 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput\n",
    "\n",
    "# Load the flow processing input.\n",
    "processing_inputs = []\n",
    "flow_input = ProcessingInput(input_name='flow', source=data_flow_uri, destination='/opt/ml/processing/flow')\n",
    "processing_inputs.append(flow_input)\n",
    "\n",
    "# Load S3 processing inputs.\n",
    "for node in flow[\"nodes\"]:\n",
    "    if \"dataset_definition\" in node[\"parameters\"]:\n",
    "        dataset_def = node[\"parameters\"][\"dataset_definition\"]\n",
    "        name = dataset_def['name']\n",
    "        source_type = dataset_def[\"datasetSourceType\"]\n",
    "        \n",
    "        if source_type == \"S3\":\n",
    "            s3_processing_input = ProcessingInput(input_name=name, \n",
    "                                                  source=dataset_def[\"s3ExecutionContext\"][\"s3Uri\"], \n",
    "                                                  destination='/opt/ml/processing/{0}'.format(name))\n",
    "            processing_inputs.append(s3_processing_input)\n",
    "\n",
    "            \n",
    "processing_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the processing outputs. We need to add a feature store output, where the name corresponds to the output name of the node in the data flow we want transformed data to be exported from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingOutput, FeatureStoreOutput\n",
    "\n",
    "processing_outputs = []\n",
    "processing_output = ProcessingOutput(output_name='e8277ec0-4c16-4469-ad66-3229508a2f20.default',\n",
    "                                     feature_store_output=FeatureStoreOutput(feature_group_name=feature_group_name),\n",
    "                                     app_managed=True)\n",
    "processing_outputs.append(processing_output)\n",
    "\n",
    "processing_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the processing job (~20 mins to complete).\n",
    "\n",
    "Note that we stop getting logs since logging is quite verbose, but you can still review all logs from Amazon CloudWatch logs. To do this you may go to the [Amazon SageMaker console](console.aws.amazon.com/sagemaker/) -> Processing -> Processing jobs -> select the latest job in progress -> Monitoring -> View logs -> click on the log 'endtoendml-load-featurestore-...'region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.run(inputs=processing_inputs,\n",
    "              outputs=processing_outputs,\n",
    "              logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features for training¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to extract features for training, by reading them from the Amazon SageMaker Feature Store offline store. We will run a SageMaker Processing job that will run an Amazon Athena query to read data from the feature store; then, we are going to transform this data to CSV for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='sklearn',\n",
    "    region=region,\n",
    "    version='0.20.0',\n",
    "    py_version='py3',\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    image_scope='training'\n",
    ")\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(role=role,\n",
    "                                     base_job_name='end-to-end-ml-sm-proc-fs',\n",
    "                                     instance_type='ml.m5.large',\n",
    "                                     instance_count=1,\n",
    "                                     framework_version='0.20.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group.athena_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.dataset_definition import DatasetDefinition, AthenaDatasetDefinition\n",
    "\n",
    "train_data_path = 's3://{0}/{1}/data/preprocessed/train/'.format(bucket_name, prefix)\n",
    "val_data_path = 's3://{0}/{1}/data/preprocessed/val/'.format(bucket_name, prefix)\n",
    "\n",
    "query_string = f'SELECT \"breakdown\",\"wind_speed\",\"rpm_blade\",\"oil_temperature\",\"oil_level\",\"temperature\",\\\n",
    "                \"humidity\",\"vibrations_frequency\",\"pressure\",\"turbine_id_tid004\",\"turbine_id_tid001\",\"turbine_id_tid006\",\\\n",
    "                \"turbine_id_tid008\",\"turbine_id_tid002\",\"turbine_id_tid003\",\"turbine_id_tid005\",\"turbine_id_tid009\",\\\n",
    "                \"turbine_id_tid010\",\"turbine_id_tid007\",\"turbine_type_hawt\",\"turbine_type_vawt\",\"wind_direction_s\",\\\n",
    "                \"wind_direction_n\",\"wind_direction_w\",\"wind_direction_sw\",\"wind_direction_e\",\"wind_direction_se\",\\\n",
    "                \"wind_direction_ne\",\"wind_direction_nw\" \\\n",
    "                FROM \"{feature_group.athena_query().database}\".\"{feature_group.athena_query().table_name}\";'\n",
    "\n",
    "featurestore_input = ProcessingInput(\n",
    "    input_name=\"features_input\",\n",
    "    app_managed=False,\n",
    "    dataset_definition=DatasetDefinition(\n",
    "        local_path=\"/opt/ml/processing/features\",\n",
    "        data_distribution_type=\"FullyReplicated\",\n",
    "        input_mode=\"File\",\n",
    "        athena_dataset_definition=AthenaDatasetDefinition(\n",
    "            catalog=feature_group.athena_query().catalog,\n",
    "            database=feature_group.athena_query().database,\n",
    "            query_string=query_string,\n",
    "            output_s3_uri='s3://{0}/{1}/tempathena'.format(bucket_name, prefix),\n",
    "            output_format=\"TEXTFILE\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "sklearn_processor.run(code='source_dir/preprocessor_dw_fs.py',\n",
    "                      inputs=[featurestore_input],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data', source='/opt/ml/processing/train', destination=train_data_path),\n",
    "                               ProcessingOutput(output_name='val_data', source='/opt/ml/processing/val', destination=val_data_path)],\n",
    "                      arguments=['--train-test-split-ratio', '0.2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_df = pd.read_csv(train_data_path + 'train_features.csv')\n",
    "train_labels_df = pd.read_csv(train_data_path + 'train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Tranformation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we cannot yet use the `.flow` file generated by DataWrangler at inference time.\n",
    "\n",
    "Therefore, we will write the same preprocessing and feature engineering code in `source_dir/preprocessor_dw_fs_model.py` and fit a SKLearn model on the trasnformations we defined, so that at inference time we can invoke it to transform the data before it is fed to the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize source_dir/preprocessor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring an Amazon SageMaker Processing job through the SM Python SDK requires to create a `Processor` object (in this case `SKLearnProcessor` as we are using the default SKLearn container for processing); we can specify how many instances we are going to use and what instance type is requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(role=role,\n",
    "                                     base_job_name='end-to-end-ml-sm-proc',\n",
    "                                     instance_type='ml.m5.large',\n",
    "                                     instance_count=1,\n",
    "                                     framework_version='0.20.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can invoke the `run()` method of the `Processor` object to kick-off the job, specifying the script to execute and the configuration of inputs and outputs.\n",
    "\n",
    "It takes ~5 mins for the job to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = 's3://{0}/{1}/data/raw/'.format(bucket_name, prefix)\n",
    "train_data_path = 's3://{0}/{1}/data/preprocessed/train/'.format(bucket_name, prefix)\n",
    "val_data_path = 's3://{0}/{1}/data/preprocessed/val/'.format(bucket_name, prefix)\n",
    "model_path = 's3://{0}/{1}/output/sklearn/'.format(bucket_name, prefix)\n",
    "\n",
    "# Experiment tracking configuration\n",
    "experiment_config={\n",
    "    \"ExperimentName\": current_experiment.experiment_name,\n",
    "    \"TrialName\": current_trial.trial_name,\n",
    "    \"TrialComponentDisplayName\": \"sklearn-preprocessing\",\n",
    "}\n",
    "\n",
    "sklearn_processor.run(code='source_dir/preprocessor.py',\n",
    "                      inputs=[ProcessingInput(input_name='raw_data', source=raw_data_path, destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data', source='/opt/ml/processing/train', destination=train_data_path),\n",
    "                               ProcessingOutput(output_name='val_data', source='/opt/ml/processing/val', destination=val_data_path),\n",
    "                               ProcessingOutput(output_name='model', source='/opt/ml/processing/model', destination=model_path)],\n",
    "                      arguments=['--train-test-split-ratio', '0.2'],\n",
    "                      experiment_config=experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
