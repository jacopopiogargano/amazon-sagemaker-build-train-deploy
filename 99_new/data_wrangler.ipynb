{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring data and storing features\n",
    "### [Amazon SageMaker Data Wranger](https://aws.amazon.com/sagemaker/data-wrangler/) & [Amazon SageMaker Feature Store](https://aws.amazon.com/sagemaker/feature-store/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the needed datasets, the next step in the Machine Learning workflow is to explore and preprocess data.\n",
    "\n",
    "In this notebook we will see how to run an Amazon SageMaker Data Wrangler job (implemented via Amazon SageMaker Processing) that will execute the transformations defined in the Data Wrangler flow and export the transformed data to Amazon SageMaker Feature Store.\n",
    "\n",
    "To do so we:\n",
    "1. create a feature group in Amazon Feature Store, to store the features describing the records;\n",
    "2. enable both offline and online feature store\n",
    "3. explore the data through Amazon SageMaker Data Wrangler\n",
    "4. define the data transformations and download the resulting `.flow` file\n",
    "5. run a Processing job to transform the data through the transformations defined in the `.flow` file\n",
    "6. output the transformed data to the feature group defined at 1.\n",
    "7. read the features from Amazon SageMaker Feature Store offline store through Amazon Athena\n",
    "8. output the features as CSV on S3 to be later used at training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.57.0\n"
     ]
    }
   ],
   "source": [
    "# Check SageMaker Python SDK version\n",
    "import sagemaker\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "def versiontuple(v):\n",
    "    return tuple(map(int, (v.split(\".\"))))\n",
    "\n",
    "if versiontuple(sagemaker.__version__) < versiontuple('2.22.0'):\n",
    "    raise Exception(\"This notebook requires at least SageMaker Python SDK version 2.22.0. Please install it via pip.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n",
      "arn:aws:iam::996912938507:role/service-role/AmazonSageMaker-ExecutionRole-endtoendml\n",
      "sagemaker-us-east-1-996912938507\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'endtoendmlsm'\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Feature Store\n",
    "\n",
    "First, let's create the target feature group in Amazon SageMaker Feature Store. A feature group is a logical grouping of features, defined in the feature store, to describe records. A feature group’s definition is composed of a list of feature definitions, a record identifier name, and configurations for its online and offline store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endtoendml-feature-group-1632177744\n",
      "Stored 'feature_group_name' (str)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "feature_group_name = 'endtoendml-feature-group-{0}'.format(str(int(time.time())))\n",
    "print(feature_group_name)\n",
    "\n",
    "%store feature_group_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the schema for the feature group, by using an empty Pandas data frame. You can also infer it by reading some data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JACOPO: perché qui non leggiamo lo schema tramite Glue?\n",
    "import pandas as pd\n",
    "\n",
    "df_columns = [\"breakdown\", \"wind_speed\", \"rpm_blade\", \"oil_temperature\", \"oil_level\", \"temperature\", \"humidity\", \n",
    "              \"vibrations_frequency\", \"pressure\", \"turbine_id_TID004\", \"turbine_id_TID001\", \"turbine_id_TID006\", \"turbine_id_TID008\", \n",
    "              \"turbine_id_TID002\", \"turbine_id_TID003\", \"turbine_id_TID005\", \"turbine_id_TID009\", \"turbine_id_TID010\", \"turbine_id_TID007\",\n",
    "              \"turbine_type_HAWT\",\"turbine_type_VAWT\", \"wind_direction_S\", \"wind_direction_N\", \"wind_direction_W\", \"wind_direction_SW\", \n",
    "              \"wind_direction_E\", \"wind_direction_SE\", \"wind_direction_NE\", \"wind_direction_NW\", \"record_id\", \"event_timestamp\"]\n",
    "df_schema = pd.DataFrame(columns=df_columns, dtype=float)\n",
    "# explicitly specifying data types for variables that are not floats\n",
    "df_schema = df_schema.astype({'wind_speed': 'long', 'rpm_blade': 'long', 'oil_level': 'long', 'temperature': 'long', 'humidity': 'long',\n",
    "                             'vibrations_frequency': 'long', 'pressure': 'long', 'record_id': 'long', 'event_timestamp': 'string'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the feature group specifying its name and the SageMaker session, and then we specify the schema for the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatureDefinition(feature_name='breakdown', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='wind_speed', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n",
       " FeatureDefinition(feature_name='rpm_blade', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n",
       " FeatureDefinition(feature_name='oil_temperature', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='oil_level', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n",
       " FeatureDefinition(feature_name='temperature', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n",
       " FeatureDefinition(feature_name='humidity', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n",
       " FeatureDefinition(feature_name='vibrations_frequency', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n",
       " FeatureDefinition(feature_name='pressure', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n",
       " FeatureDefinition(feature_name='turbine_id_TID004', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='turbine_id_TID001', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='turbine_id_TID006', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='turbine_id_TID008', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='turbine_id_TID002', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='turbine_id_TID003', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='turbine_id_TID005', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='turbine_id_TID009', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='turbine_id_TID010', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='turbine_id_TID007', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='turbine_type_HAWT', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='turbine_type_VAWT', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='wind_direction_S', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='wind_direction_N', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='wind_direction_W', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='wind_direction_SW', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='wind_direction_E', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='wind_direction_SE', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='wind_direction_NE', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='wind_direction_NW', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='record_id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n",
       " FeatureDefinition(feature_name='event_timestamp', feature_type=<FeatureTypeEnum.STRING: 'String'>)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.feature_store import feature_group\n",
    "\n",
    "feature_group = feature_group.FeatureGroup(name=feature_group_name,\n",
    "                                           sagemaker_session = sagemaker_session)\n",
    "\n",
    "feature_group.load_feature_definitions(df_schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create the feature store; we will enable both online and offline store for this example.\n",
    "\n",
    "_Online store_: used for low latency real-time inference use cases (low millisecond latency reads and high throughput writes).\n",
    "\n",
    "_Offline store_: used for training and batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:996912938507:feature-group/endtoendml-feature-group-1632177744',\n",
       " 'ResponseMetadata': {'RequestId': '406aa8ba-1095-4f8d-bfef-11baa66ff495',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '406aa8ba-1095-4f8d-bfef-11baa66ff495',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '112',\n",
       "   'date': 'Mon, 20 Sep 2021 22:42:25 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we specify an s3 location for the offline feature store.\n",
    "offline_store_uri = 's3://{0}/{1}/feature_store'.format(bucket_name, prefix)\n",
    "\n",
    "feature_group.create(s3_uri=offline_store_uri,\n",
    "                     record_identifier_name='record_id',\n",
    "                     event_time_feature_name='event_timestamp',\n",
    "                     role_arn=role,\n",
    "                     enable_online_store=True,\n",
    "                     description='Wind turbine features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait a few seconds for the feature group to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    status = feature_group.describe()['FeatureGroupStatus']\n",
    "    print(status)\n",
    "    if status == 'Created':\n",
    "        break;\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Data Wrangler\n",
    "\n",
    "Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high quality models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, follow these steps:\n",
    "1. In the left menu, go to SageMaker resources (orange triangle shape)\n",
    "2. Select 'Data Wrangler'\n",
    "3. Create a New Flow and click on Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/wrangler_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Import the data after briefly inspecting it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/wrangler_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Click on the + symbol on the right and add an Analysis to explore the data through Data Wrangler's features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/wrangler_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. For instance, you may choose the Histogram visualization and plot the `wind_speed` distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/wrangler_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Go back to the data flow and add a Transform. There are many pre built transforms to choose from, plus you can bring your own transform or formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/wrangler_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. As an example, choose `Handle Missing` -> `Fill missing` -> `turbine_type` -> `HAWT`, and preview the transformation by clicking on __Preview__. The missing values in the column `turbine_type` were filled in with the string `HAWT`. If you are satisfied of the results, you can add the transform step to the transformation pipeline by clicking on __Add__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/wrangler_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. After you have added all the needed steps, you are all set. You may explore the `.flow` generated file in your local SageMaker repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/wrangler_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to use an Amazon SageMaker Data Wrangler job, implemented as a SageMaker Processing job, to interpret the data flow defined with Amazon SageMaker Data Wrangler and load the transformed data to the feature group previously created.\n",
    "\n",
    "First thing to do is uploading the data flow to Amazon S3, since it will be used as input to the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-996912938507/endtoendmlsm/data_flow/data_exploration.flow\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "flow_file_name = 'data_exploration.flow'\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "    \n",
    "data_flow_uri = 's3://{0}/{1}/data_flow/{2}'.format(bucket_name, prefix, flow_file_name)\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket_name, '{0}/data_flow/{1}'.format(prefix, flow_file_name))\n",
    "\n",
    "print(data_flow_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.x\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker import image_uris\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/ecr-us-east-1.html#data-wrangler-us-east-1.title\n",
    "\n",
    "data_wrangler_image_uri = image_uris.retrieve(framework='data-wrangler',region=region)\n",
    "\n",
    "processor = Processor(image_uri=data_wrangler_image_uri,\n",
    "                      role=role,\n",
    "                      instance_count=1,\n",
    "                      instance_type='ml.m5.4xlarge',\n",
    "                      base_job_name='endtoendml-load-featurestore',\n",
    "                      sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the inputs for the Data Wrangler job. It expects the flow definition and all dataset definitions used to laod data in the flow. In this scenario, we only accessed a dataset from S3, so we are going to parse only S3 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<sagemaker.processing.ProcessingInput at 0x7f2041289e10>,\n",
       " <sagemaker.processing.ProcessingInput at 0x7f2041289ad0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput\n",
    "\n",
    "# Load the flow processing input.\n",
    "processing_inputs = []\n",
    "flow_input = ProcessingInput(input_name='flow', source=data_flow_uri, destination='/opt/ml/processing/flow')\n",
    "processing_inputs.append(flow_input)\n",
    "\n",
    "# Load S3 processing inputs.\n",
    "for node in flow[\"nodes\"]:\n",
    "    if \"dataset_definition\" in node[\"parameters\"]:\n",
    "        dataset_def = node[\"parameters\"][\"dataset_definition\"]\n",
    "        name = dataset_def['name']\n",
    "        source_type = dataset_def[\"datasetSourceType\"]\n",
    "        \n",
    "        if source_type == \"S3\":\n",
    "            s3_processing_input = ProcessingInput(input_name=name, \n",
    "                                                  source=dataset_def[\"s3ExecutionContext\"][\"s3Uri\"], \n",
    "                                                  destination='/opt/ml/processing/{0}'.format(name))\n",
    "            processing_inputs.append(s3_processing_input)\n",
    "\n",
    "            \n",
    "processing_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the processing outputs. We need to add a feature store output, where the name corresponds to the output name of the node in the data flow we want transformed data to be exported from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<sagemaker.processing.ProcessingOutput at 0x7f203e727d50>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput, FeatureStoreOutput\n",
    "\n",
    "processing_outputs = []\n",
    "processing_output = ProcessingOutput(output_name='e8277ec0-4c16-4469-ad66-3229508a2f20.default',\n",
    "                                     feature_store_output=FeatureStoreOutput(feature_group_name=feature_group_name),\n",
    "                                     app_managed=True)\n",
    "processing_outputs.append(processing_output)\n",
    "\n",
    "processing_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the processing job (~20 mins to complete).\n",
    "\n",
    "Note that we stop getting logs since logging is quite verbose, but you can still review all logs from Amazon CloudWatch logs. To do this you may go to the [Amazon SageMaker console](console.aws.amazon.com/sagemaker/) -> Processing -> Processing jobs -> select the latest job in progress -> Monitoring -> View logs -> click on the log 'endtoendml-load-featurestore-...'region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.run(inputs=processing_inputs,\n",
    "              outputs=processing_outputs,\n",
    "              logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features for training¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to extract features for training, by reading them from the Amazon SageMaker Feature Store offline store. We will run a SageMaker Processing job that will run an Amazon Athena query to read data from the feature store; then, we are going to transform this data to CSV for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='sklearn',\n",
    "    region=region,\n",
    "    version='0.20.0',\n",
    "    py_version='py3',\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    image_scope='training'\n",
    ")\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(role=role,\n",
    "                                     base_job_name='end-to-end-ml-sm-proc-fs',\n",
    "                                     instance_type='ml.m5.large',\n",
    "                                     instance_count=1,\n",
    "                                     framework_version='0.20.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AthenaQuery(catalog='AwsDataCatalog', database='sagemaker_featurestore', table_name='endtoendml-feature-group-1632177744-1632177746', sagemaker_session=<sagemaker.session.Session object at 0x7f2040fb5550>, _current_query_execution_id=None, _result_bucket=None, _result_file_prefix=None)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group.athena_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  end-to-end-ml-sm-proc-fs-2021-09-20-22-53-54-421\n",
      "Inputs:  [{'InputName': 'features_input', 'AppManaged': False, 'DatasetDefinition': {'LocalPath': '/opt/ml/processing/features', 'DataDistributionType': 'FullyReplicated', 'InputMode': 'File', 'AthenaDatasetDefinition': {'Catalog': 'AwsDataCatalog', 'Database': 'sagemaker_featurestore', 'QueryString': 'SELECT \"breakdown\",\"wind_speed\",\"rpm_blade\",\"oil_temperature\",\"oil_level\",\"temperature\",                \"humidity\",\"vibrations_frequency\",\"pressure\",\"turbine_id_tid004\",\"turbine_id_tid001\",\"turbine_id_tid006\",                \"turbine_id_tid008\",\"turbine_id_tid002\",\"turbine_id_tid003\",\"turbine_id_tid005\",\"turbine_id_tid009\",                \"turbine_id_tid010\",\"turbine_id_tid007\",\"turbine_type_hawt\",\"turbine_type_vawt\",\"wind_direction_s\",                \"wind_direction_n\",\"wind_direction_w\",\"wind_direction_sw\",\"wind_direction_e\",\"wind_direction_se\",                \"wind_direction_ne\",\"wind_direction_nw\"                 FROM \"sagemaker_featurestore\".\"endtoendml-feature-group-1632177744-1632177746\";', 'OutputS3Uri': 's3://sagemaker-us-east-1-996912938507/endtoendmlsm/tempathena', 'OutputFormat': 'TEXTFILE'}}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-996912938507/end-to-end-ml-sm-proc-fs-2021-09-20-22-53-54-421/input/code/preprocessor.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-996912938507/endtoendmlsm/data/preprocessed/train/', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'val_data', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-996912938507/endtoendmlsm/data/preprocessed/val/', 'LocalPath': '/opt/ml/processing/val', 'S3UploadMode': 'EndOfJob'}}]\n",
      "........................\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T22:57:47.477Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Athena dataset definition specified. Starting athena query execution.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T22:57:47.477Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Creating database 'sagemaker_processing' in catalog 'awsdatacatalog' if doesn't exist already.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T22:57:47.673Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Database 'sagemaker_processing' already exists in catalog 'awsdatacatalog'.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T22:57:47.673Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Starting Athena query execution.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T22:57:47.778Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Waiting for query execution to complete, QueryExecutionId: c9c8def6-de6d-428d-9cfd-a90298d984be\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T22:57:47.796Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Waiting for query execution to complete, QueryExecutionId: c9c8def6-de6d-428d-9cfd-a90298d984be\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T22:57:52.813Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Query execution completed successfully.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T22:57:52.813Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Preparing to clean up resources.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T22:57:53.287Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Resource clean up complete. Exiting.\"}\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Processing job end-to-end-ml-sm-proc-fs-2021-09-20-22-53-54-421: Failed. Reason: ClientError: Failed to download data. S3 key: s3://sagemaker-us-east-1-996912938507/endtoendmlsm/tempathena/end-to-end-ml-sm-proc-fs-2021-09-20-22-53-54-421/data matched no files on s3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-2b177074c50b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m                       outputs=[ProcessingOutput(output_name='train_data', source='/opt/ml/processing/train', destination=train_data_path),\n\u001b[1;32m     33\u001b[0m                                ProcessingOutput(output_name='val_data', source='/opt/ml/processing/val', destination=val_data_path)],\n\u001b[0;32m---> 34\u001b[0;31m                       arguments=['--train-test-split-ratio', '0.2'])\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, code, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_include_code_in_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    930\u001b[0m         \"\"\"\n\u001b[1;32m    931\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_processing_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   3763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3765\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ProcessingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3766\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3767\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3234\u001b[0m                 ),\n\u001b[1;32m   3235\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m             )\n\u001b[1;32m   3238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Processing job end-to-end-ml-sm-proc-fs-2021-09-20-22-53-54-421: Failed. Reason: ClientError: Failed to download data. S3 key: s3://sagemaker-us-east-1-996912938507/endtoendmlsm/tempathena/end-to-end-ml-sm-proc-fs-2021-09-20-22-53-54-421/data matched no files on s3"
     ]
    }
   ],
   "source": [
    "from sagemaker.dataset_definition import DatasetDefinition, AthenaDatasetDefinition\n",
    "\n",
    "train_data_path = 's3://{0}/{1}/data/preprocessed/train/'.format(bucket_name, prefix)\n",
    "val_data_path = 's3://{0}/{1}/data/preprocessed/val/'.format(bucket_name, prefix)\n",
    "\n",
    "query_string = f'SELECT \"breakdown\",\"wind_speed\",\"rpm_blade\",\"oil_temperature\",\"oil_level\",\"temperature\",\\\n",
    "                \"humidity\",\"vibrations_frequency\",\"pressure\",\"turbine_id_tid004\",\"turbine_id_tid001\",\"turbine_id_tid006\",\\\n",
    "                \"turbine_id_tid008\",\"turbine_id_tid002\",\"turbine_id_tid003\",\"turbine_id_tid005\",\"turbine_id_tid009\",\\\n",
    "                \"turbine_id_tid010\",\"turbine_id_tid007\",\"turbine_type_hawt\",\"turbine_type_vawt\",\"wind_direction_s\",\\\n",
    "                \"wind_direction_n\",\"wind_direction_w\",\"wind_direction_sw\",\"wind_direction_e\",\"wind_direction_se\",\\\n",
    "                \"wind_direction_ne\",\"wind_direction_nw\" \\\n",
    "                FROM \"{feature_group.athena_query().database}\".\"{feature_group.athena_query().table_name}\";'\n",
    "\n",
    "featurestore_input = ProcessingInput(\n",
    "    input_name=\"features_input\",\n",
    "    app_managed=False,\n",
    "    dataset_definition=DatasetDefinition(\n",
    "        local_path=\"/opt/ml/processing/features\",\n",
    "        data_distribution_type=\"FullyReplicated\",\n",
    "        input_mode=\"File\",\n",
    "        athena_dataset_definition=AthenaDatasetDefinition(\n",
    "            catalog=feature_group.athena_query().catalog,\n",
    "            database=feature_group.athena_query().database,\n",
    "            query_string=query_string,\n",
    "            output_s3_uri='s3://{0}/{1}/tempathena'.format(bucket_name, prefix),\n",
    "            output_format=\"TEXTFILE\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "sklearn_processor.run(code='preprocessor.py',\n",
    "                      inputs=[featurestore_input],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data', source='/opt/ml/processing/train', destination=train_data_path),\n",
    "                               ProcessingOutput(output_name='val_data', source='/opt/ml/processing/val', destination=val_data_path)],\n",
    "                      arguments=['--train-test-split-ratio', '0.2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  end-to-end-ml-sm-proc-fs-2021-09-20-23-01-23-897\n",
      "Inputs:  [{'InputName': 'features_input', 'AppManaged': False, 'DatasetDefinition': {'LocalPath': '/opt/ml/processing/features', 'DataDistributionType': 'FullyReplicated', 'InputMode': 'File', 'AthenaDatasetDefinition': {'Catalog': 'AwsDataCatalog', 'Database': 'sagemaker_featurestore', 'QueryString': 'SELECT \"breakdown\",\"wind_speed\",\"rpm_blade\",\"oil_temperature\",\"oil_level\",\"temperature\",                \"humidity\",\"vibrations_frequency\",\"pressure\",\"turbine_id_tid004\",\"turbine_id_tid001\",\"turbine_id_tid006\",                \"turbine_id_tid008\",\"turbine_id_tid002\",\"turbine_id_tid003\",\"turbine_id_tid005\",\"turbine_id_tid009\",                \"turbine_id_tid010\",\"turbine_id_tid007\",\"turbine_type_hawt\",\"turbine_type_vawt\",\"wind_direction_s\",                \"wind_direction_n\",\"wind_direction_w\",\"wind_direction_sw\",\"wind_direction_e\",\"wind_direction_se\",                \"wind_direction_ne\",\"wind_direction_nw\"                 FROM \"sagemaker_featurestore\".\"endtoendml-feature-group-1632172794-1632172946\";', 'OutputS3Uri': 's3://sagemaker-us-east-1-996912938507/endtoendmlsm/tempathena', 'OutputFormat': 'TEXTFILE'}}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-996912938507/end-to-end-ml-sm-proc-fs-2021-09-20-23-01-23-897/input/code/preprocessor.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-996912938507/endtoendmlsm/data/preprocessed/train/', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'val_data', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-996912938507/endtoendmlsm/data/preprocessed/val/', 'LocalPath': '/opt/ml/processing/val', 'S3UploadMode': 'EndOfJob'}}]\n",
      "........................\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T23:05:10.765Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Athena dataset definition specified. Starting athena query execution.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T23:05:10.766Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Creating database 'sagemaker_processing' in catalog 'awsdatacatalog' if doesn't exist already.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T23:05:10.974Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Database 'sagemaker_processing' already exists in catalog 'awsdatacatalog'.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T23:05:10.974Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Starting Athena query execution.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T23:05:11.203Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Waiting for query execution to complete, QueryExecutionId: e4215a64-0428-4483-a2c8-98e74183e216\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T23:05:11.223Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Waiting for query execution to complete, QueryExecutionId: e4215a64-0428-4483-a2c8-98e74183e216\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T23:05:16.242Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Query execution completed successfully.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T23:05:16.242Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Preparing to clean up resources.\"}\u001b[0m\n",
      "\u001b[34m{\"level\":\"INFO\",\"ts\":\"2021-09-20T23:05:16.704Z\",\"msg\":\"[sagemaker logs] [Input: features_input] Resource clean up complete. Exiting.\"}\u001b[0m\n",
      "\u001b[34mUnzipping Athena results\u001b[0m\n",
      "\u001b[35mUnzipping Athena results\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m['/opt/ml/processing/features/20210920_230511_00093_jrc22_e4c22db5-f47b-4c09-abcb-bd5654240e7b', '/opt/ml/processing/features/20210920_230511_00093_jrc22_26902e1e-ea25-4bf5-be78-e1b82c449045', '/opt/ml/processing/features/20210920_230511_00093_jrc22_3d852b33-ff85-4b61-a476-d50aa29649f6', '/opt/ml/processing/features/20210920_230511_00093_jrc22_04e05836-4ccc-4849-b7a1-123065a610f4', '/opt/ml/processing/features/20210920_230511_00093_jrc22_8270f355-aa15-4ee0-a7b7-dbbf3d371aae', '/opt/ml/processing/features/20210920_230511_00093_jrc22_da2b6963-51cc-4bf2-bc0a-6fb2a49a5692', '/opt/ml/processing/features/20210920_230511_00093_jrc22_b118d0b5-e1b4-4b0a-9207-d60bced920af', '/opt/ml/processing/features/20210920_230511_00093_jrc22_999d8a67-ecdb-4321-b7ef-5a29d81b2118', '/opt/ml/processing/features/20210920_230511_00093_jrc22_48e75fc3-54b8-4bb1-a6f9-7cac6ef019d6', '/opt/ml/processing/features/20210920_230511_00093_jrc22_adc4e457-72f7-4914-b4df-82dbfbcb993f', '/opt/ml/processing/features/20210920_230511_00093_jrc22_49c0b28b-65de-46a8-bc37-15b1b8fd162a', '/opt/ml/processing/features/20210920_230511_00093_jrc22_79e3ad62-469e-40fd-b5bc-43a079e865f5', '/opt/ml/processing/features/20210920_230511_00093_jrc22_3f7fb85c-7815-406c-86a6-649923dd12e0', '/opt/ml/processing/features/20210920_230511_00093_jrc22_418e7f5a-e266-47e8-a2ec-04150a08ae1a', '/opt/ml/processing/features/20210920_230511_00093_jrc22_9fed7aee-1df1-4ae9-9ffb-af3c1c3aa546', '/opt/ml/processing/features/20210920_230511_00093_jrc22_bc36abd4-762f-45bd-8bb8-ee863695e4c5', '/opt/ml/processing/features/20210920_230511_00093_jrc22_e1537e90-b959-4cd7-bc1b-2530b0247161', '/opt/ml/processing/features/20210920_230511_00093_jrc22_6a86c0c7-1d0e-403c-802c-97004bffef69', '/opt/ml/processing/features/20210920_230511_00093_jrc22_352a575a-547f-45aa-ba70-23311f941e9f', '/opt/ml/processing/features/20210920_230511_00093_jrc22_905a71cf-e47b-40f3-b905-11006e135aae', '/opt/ml/processing/features/20210920_230511_00093_jrc22_6895a9d5-b82b-45a3-814f-c72f4f03dc03', '/opt/ml/processing/features/20210920_230511_00093_jrc22_6a052505-cb50-4c33-9a52-5661678b16f4', '/opt/ml/processing/features/20210920_230511_00093_jrc22_d0bd406f-de47-4825-84fc-77c1c457411d', '/opt/ml/processing/features/20210920_230511_00093_jrc22_1cf80550-fdc0-440f-adff-97ca9799a21e', '/opt/ml/processing/features/20210920_230511_00093_jrc22_35e3017a-10e6-46fe-8e55-7c9c6ab8d5d6', '/opt/ml/processing/features/20210920_230511_00093_jrc22_dc30ab08-df36-42d1-bc04-5e9a570917d3', '/opt/ml/processing/features/20210920_230511_00093_jrc22_927dd8e7-4a5c-4c1f-b9d0-c73c9239ebf6', '/opt/ml/processing/features/20210920_230511_00093_jrc22_b78c780c-d006-4f5e-8309-e4b561015629', '/opt/ml/processing/features/20210920_230511_00093_jrc22_af76f933-1e94-492d-be67-afac6f8d3262', '/opt/ml/processing/features/20210920_230511_00093_jrc22_97849268-3022-4a50-ab3a-6ee69b13feed']\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m['/opt/ml/processing/features/20210920_230511_00093_jrc22_e4c22db5-f47b-4c09-abcb-bd5654240e7b', '/opt/ml/processing/features/20210920_230511_00093_jrc22_26902e1e-ea25-4bf5-be78-e1b82c449045', '/opt/ml/processing/features/20210920_230511_00093_jrc22_3d852b33-ff85-4b61-a476-d50aa29649f6', '/opt/ml/processing/features/20210920_230511_00093_jrc22_04e05836-4ccc-4849-b7a1-123065a610f4', '/opt/ml/processing/features/20210920_230511_00093_jrc22_8270f355-aa15-4ee0-a7b7-dbbf3d371aae', '/opt/ml/processing/features/20210920_230511_00093_jrc22_da2b6963-51cc-4bf2-bc0a-6fb2a49a5692', '/opt/ml/processing/features/20210920_230511_00093_jrc22_b118d0b5-e1b4-4b0a-9207-d60bced920af', '/opt/ml/processing/features/20210920_230511_00093_jrc22_999d8a67-ecdb-4321-b7ef-5a29d81b2118', '/opt/ml/processing/features/20210920_230511_00093_jrc22_48e75fc3-54b8-4bb1-a6f9-7cac6ef019d6', '/opt/ml/processing/features/20210920_230511_00093_jrc22_adc4e457-72f7-4914-b4df-82dbfbcb993f', '/opt/ml/processing/features/20210920_230511_00093_jrc22_49c0b28b-65de-46a8-bc37-15b1b8fd162a', '/opt/ml/processing/features/20210920_230511_00093_jrc22_79e3ad62-469e-40fd-b5bc-43a079e865f5', '/opt/ml/processing/features/20210920_230511_00093_jrc22_3f7fb85c-7815-406c-86a6-649923dd12e0', '/opt/ml/processing/features/20210920_230511_00093_jrc22_418e7f5a-e266-47e8-a2ec-04150a08ae1a', '/opt/ml/processing/features/20210920_230511_00093_jrc22_9fed7aee-1df1-4ae9-9ffb-af3c1c3aa546', '/opt/ml/processing/features/20210920_230511_00093_jrc22_bc36abd4-762f-45bd-8bb8-ee863695e4c5', '/opt/ml/processing/features/20210920_230511_00093_jrc22_e1537e90-b959-4cd7-bc1b-2530b0247161', '/opt/ml/processing/features/20210920_230511_00093_jrc22_6a86c0c7-1d0e-403c-802c-97004bffef69', '/opt/ml/processing/features/20210920_230511_00093_jrc22_352a575a-547f-45aa-ba70-23311f941e9f', '/opt/ml/processing/features/20210920_230511_00093_jrc22_905a71cf-e47b-40f3-b905-11006e135aae', '/opt/ml/processing/features/20210920_230511_00093_jrc22_6895a9d5-b82b-45a3-814f-c72f4f03dc03', '/opt/ml/processing/features/20210920_230511_00093_jrc22_6a052505-cb50-4c33-9a52-5661678b16f4', '/opt/ml/processing/features/20210920_230511_00093_jrc22_d0bd406f-de47-4825-84fc-77c1c457411d', '/opt/ml/processing/features/20210920_230511_00093_jrc22_1cf80550-fdc0-440f-adff-97ca9799a21e', '/opt/ml/processing/features/20210920_230511_00093_jrc22_35e3017a-10e6-46fe-8e55-7c9c6ab8d5d6', '/opt/ml/processing/features/20210920_230511_00093_jrc22_dc30ab08-df36-42d1-bc04-5e9a570917d3', '/opt/ml/processing/features/20210920_230511_00093_jrc22_927dd8e7-4a5c-4c1f-b9d0-c73c9239ebf6', '/opt/ml/processing/features/20210920_230511_00093_jrc22_b78c780c-d006-4f5e-8309-e4b561015629', '/opt/ml/processing/features/20210920_230511_00093_jrc22_af76f933-1e94-492d-be67-afac6f8d3262', '/opt/ml/processing/features/20210920_230511_00093_jrc22_97849268-3022-4a50-ab3a-6ee69b13feed']\u001b[0m\n",
      "\u001b[34mSplitting data into train and validation sets with ratio 0.2\u001b[0m\n",
      "\u001b[35mSplitting data into train and validation sets with ratio 0.2\u001b[0m\n",
      "\u001b[34mTrain features shape after preprocessing: (1600000, 28)\u001b[0m\n",
      "\u001b[34mValidation features shape after preprocessing: (400000, 28)\u001b[0m\n",
      "\u001b[34mSaving training features to /opt/ml/processing/train/train_features.csv\u001b[0m\n",
      "\u001b[35mTrain features shape after preprocessing: (1600000, 28)\u001b[0m\n",
      "\u001b[35mValidation features shape after preprocessing: (400000, 28)\u001b[0m\n",
      "\u001b[35mSaving training features to /opt/ml/processing/train/train_features.csv\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.dataset_definition import DatasetDefinition, AthenaDatasetDefinition\n",
    "\n",
    "train_data_path = 's3://{0}/{1}/data/preprocessed/train/'.format(bucket_name, prefix)\n",
    "val_data_path = 's3://{0}/{1}/data/preprocessed/val/'.format(bucket_name, prefix)\n",
    "\n",
    "query_string = f'SELECT \"breakdown\",\"wind_speed\",\"rpm_blade\",\"oil_temperature\",\"oil_level\",\"temperature\",\\\n",
    "                \"humidity\",\"vibrations_frequency\",\"pressure\",\"turbine_id_tid004\",\"turbine_id_tid001\",\"turbine_id_tid006\",\\\n",
    "                \"turbine_id_tid008\",\"turbine_id_tid002\",\"turbine_id_tid003\",\"turbine_id_tid005\",\"turbine_id_tid009\",\\\n",
    "                \"turbine_id_tid010\",\"turbine_id_tid007\",\"turbine_type_hawt\",\"turbine_type_vawt\",\"wind_direction_s\",\\\n",
    "                \"wind_direction_n\",\"wind_direction_w\",\"wind_direction_sw\",\"wind_direction_e\",\"wind_direction_se\",\\\n",
    "                \"wind_direction_ne\",\"wind_direction_nw\" \\\n",
    "                FROM \"{feature_group.athena_query().database}\".\"endtoendml-feature-group-1632172794-1632172946\";'\n",
    "\n",
    "featurestore_input = ProcessingInput(\n",
    "    input_name=\"features_input\",\n",
    "    app_managed=False,\n",
    "    dataset_definition=DatasetDefinition(\n",
    "        local_path=\"/opt/ml/processing/features\",\n",
    "        data_distribution_type=\"FullyReplicated\",\n",
    "        input_mode=\"File\",\n",
    "        athena_dataset_definition=AthenaDatasetDefinition(\n",
    "            catalog=feature_group.athena_query().catalog,\n",
    "            database=feature_group.athena_query().database,\n",
    "            query_string=query_string,\n",
    "            output_s3_uri='s3://{0}/{1}/tempathena'.format(bucket_name, prefix),\n",
    "            output_format=\"TEXTFILE\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "sklearn_processor.run(code='preprocessor.py',\n",
    "                      inputs=[featurestore_input],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data', source='/opt/ml/processing/train', destination=train_data_path),\n",
    "                               ProcessingOutput(output_name='val_data', source='/opt/ml/processing/val', destination=val_data_path)],\n",
    "                      arguments=['--train-test-split-ratio', '0.2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_df = pd.read_csv(train_data_path + 'train_features.csv')\n",
    "train_labels_df = pd.read_csv(train_data_path + 'train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>55</th>\n",
       "      <th>15</th>\n",
       "      <th>46.0</th>\n",
       "      <th>29</th>\n",
       "      <th>81</th>\n",
       "      <th>29.1</th>\n",
       "      <th>1</th>\n",
       "      <th>50</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0.1</th>\n",
       "      <th>...</th>\n",
       "      <th>0.0.9</th>\n",
       "      <th>1.0.1</th>\n",
       "      <th>0.0.10</th>\n",
       "      <th>0.0.11</th>\n",
       "      <th>1.0.2</th>\n",
       "      <th>0.0.12</th>\n",
       "      <th>0.0.13</th>\n",
       "      <th>0.0.14</th>\n",
       "      <th>0.0.15</th>\n",
       "      <th>0.0.16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>46.0</td>\n",
       "      <td>28</td>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77</td>\n",
       "      <td>46</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6</td>\n",
       "      <td>83</td>\n",
       "      <td>77</td>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>24</td>\n",
       "      <td>44.0</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>15</td>\n",
       "      <td>41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>52</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>76</td>\n",
       "      <td>11</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>70</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>83</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>45</td>\n",
       "      <td>79</td>\n",
       "      <td>46.0</td>\n",
       "      <td>15</td>\n",
       "      <td>84</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>24</td>\n",
       "      <td>63</td>\n",
       "      <td>43.0</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>50.0</td>\n",
       "      <td>19</td>\n",
       "      <td>72</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>17</td>\n",
       "      <td>39</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6</td>\n",
       "      <td>71</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>50.0</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>51</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         55  15  46.0  29  81  29.1   1  50  0.0  0.0.1  ...  0.0.9  1.0.1  \\\n",
       "0        66  29  46.0  28  67    63  10  20  0.0    0.0  ...    1.0    0.0   \n",
       "1        77  46  30.0   6  83    77  14  29  0.0    0.0  ...    1.0    0.0   \n",
       "2        66  24  44.0  14  27    59  15  41  0.0    0.0  ...    0.0    1.0   \n",
       "3        50  52  33.0   6  77    76  11  72  0.0    0.0  ...    1.0    0.0   \n",
       "4        44  70  40.0   6  29    83   9  27  0.0    0.0  ...    1.0    0.0   \n",
       "...      ..  ..   ...  ..  ..   ...  ..  ..  ...    ...  ...    ...    ...   \n",
       "1599994  45  79  46.0  15  84    23  12  34  0.0    0.0  ...    1.0    0.0   \n",
       "1599995  24  63  43.0  25  22    35  10  80  0.0    0.0  ...    1.0    0.0   \n",
       "1599996  45  27  50.0  19  72    53   4  78  0.0    0.0  ...    1.0    0.0   \n",
       "1599997  17  39  25.0   6  71    55   7  74  0.0    1.0  ...    1.0    0.0   \n",
       "1599998  40  19  50.0  24  31    51  13  25  0.0    0.0  ...    1.0    0.0   \n",
       "\n",
       "         0.0.10  0.0.11  1.0.2  0.0.12  0.0.13  0.0.14  0.0.15  0.0.16  \n",
       "0           0.0     0.0    1.0     0.0     0.0     0.0     0.0     0.0  \n",
       "1           0.0     0.0    0.0     0.0     0.0     0.0     0.0     1.0  \n",
       "2           0.0     0.0    0.0     0.0     0.0     1.0     0.0     0.0  \n",
       "3           1.0     0.0    0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "4           0.0     0.0    0.0     0.0     0.0     0.0     1.0     0.0  \n",
       "...         ...     ...    ...     ...     ...     ...     ...     ...  \n",
       "1599994     0.0     1.0    0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "1599995     0.0     0.0    0.0     1.0     0.0     0.0     0.0     0.0  \n",
       "1599996     0.0     0.0    0.0     0.0     0.0     0.0     1.0     0.0  \n",
       "1599997     0.0     0.0    0.0     0.0     0.0     0.0     0.0     1.0  \n",
       "1599998     0.0     0.0    0.0     0.0     0.0     0.0     1.0     0.0  \n",
       "\n",
       "[1599999 rows x 28 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0.0\n",
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        1.0\n",
       "...      ...\n",
       "1599994  1.0\n",
       "1599995  0.0\n",
       "1599996  0.0\n",
       "1599997  0.0\n",
       "1599998  0.0\n",
       "\n",
       "[1599999 rows x 1 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
